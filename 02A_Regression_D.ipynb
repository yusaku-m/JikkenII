{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 環境構築\n",
    "環境構築ソフト[pixi](https://pixi.sh/ \"pixi公式\")を使用して環境構築を行う。\n",
    "1. 「ファイル」，「フォルダを開く」からこのノートブックファイルが直下に入っているフォルダを開く。zipファイルは同名フォルダが2重になっているので注意\n",
    "1. 画面下部にカーソルを持っていき，アイコンが変わったらクリックしたまま上にドラッグするとターミナルが開く（またはCtrlと`の同時押し）\n",
    "1. 開いたターミナルに，「pixi shell」と打ち込むと環境が構築できる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## インポート\n",
    "[AI開発用ライブラリpytorch](https://pytorch.org/ \"pytorch公式\")を主として，必要なライブラリをインポートする。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import pandas as pd\n",
    "import torch\n",
    "import seaborn as sns\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "条件記録変数の初期化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データの読み込み\n",
    "表形式のデータを扱うのに長けた，[pandas](https://pandas.pydata.org/ \"pandas公式\")でデータを読み込む。\n",
    "pandasにはread_csv()やread_excel()関数があり，一般的な表形式データをそのまま読み込むことができる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./data/intermittent-renewables-production-france.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jupyter Notebook形式でスクリプトを書いている場合，セルの最終行に変数名だけを記載した場合，その内容が表示される。\n",
    "その他の場所で表示がしたい場合はprint()関数の使用が必要"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# データの可視化\n",
    "import seaborn as sns\n",
    "でインポートしたseabornモジュールを使用して，各数値間の関係を可視化する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データの可視化\n",
    "#sns.pairplot(df)#, hue=\"age\")\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データの加工"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# StartHour,EndHourは頭2文字を抽出して数値化\n",
    "df[\"StartHour\"] = df[\"StartHour\"].str[:2].astype(int)\n",
    "df[\"EndHour\"] = df[\"EndHour\"].str[:2].astype(int)\n",
    "\n",
    "# Sourceを，Wind:0, Solar:1\n",
    "df[\"Source\"] = df[\"Source\"].map({\"Wind\": 0, \"Solar\": 1})\n",
    "# 列名をSource(0: Wind, 1: Solar)に変更\n",
    "df = df.rename(columns={\"Source\": \"Source(0: Wind, 1: Solar)\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ./data/02D_regression_WindAndSolarPowerPrediction.csvとして保存\n",
    "df.to_csv(\"./data/02D_regression_WindAndSolarPowerPrediction.csv\", index=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文字データは数値として入力できないので，該当列名を指定して削除\n",
    "\n",
    "df = df.drop(columns=[\n",
    "    \"Date and Hour\",\n",
    "    \"Date\",\n",
    "    \"dayName\",\n",
    "    \"monthName\"\n",
    "    ])\n",
    "#削除できたか確認\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 欠損値(nan)の確認\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 欠損値行を削除\n",
    "df.dropna(inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文字列データを数値に変換\n",
    "for column in df.select_dtypes(include=['object']).columns:\n",
    "    df[column] = df[column].astype('float')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データを訓練・テストに分割\n",
    "データ全てを使って訓練すると，過学習（問題集の丸暗記に近い状態）となり，初めて見るデータに対する推測性能を知ることができない。\n",
    "整形したデータの２割を，テストデータ，８割を訓練データとして分割する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 回帰（推定）対象を除いたデータをX，回帰対象をyとして分ける\n",
    "parameters[\"regression_terget\"] = \"Production\"\n",
    "X = df.drop(parameters.get(\"regression_terget\"), axis=1)\n",
    "y = df[parameters.get(\"regression_terget\")]\n",
    "\n",
    "# 分割の再現性を確保するため，シード値を指定したうえで訓練データ・テストデータに分割（ここは変えない）\n",
    "parameters[\"test_size\"] = 0.2\n",
    "parameters[\"random_seed\"] = 0\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=parameters.get(\"test_size\"),\n",
    "                                                    random_state=parameters.get(\"random_seed\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分割結果の確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データをpytorch用に変換\n",
    "データを変換し，モデルが読み込めるようにデータローダーを定義する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torchテンソルに変換\n",
    "X_train = torch.tensor(X_train.values, dtype=torch.float)\n",
    "X_test = torch.tensor(X_test.values, dtype=torch.float)\n",
    "y_train = torch.tensor(y_train.values, dtype=torch.float).view(-1, 1)\n",
    "y_test = torch.tensor(y_test.values, dtype=torch.float).view(-1, 1)\n",
    "\n",
    "# データローダー定義用に変換\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "# データローダー定義\n",
    "parameters[\"batch_size\"] = 128\n",
    "train_loader = DataLoader(train_dataset, batch_size=parameters.get('batch_size'), shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# テンソル形状の確認\n",
    "for batch, (X, y) in enumerate(train_loader):\n",
    "    pass\n",
    "print(f\"batch: {batch}, X: {X.shape}, y: {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデルの定義\n",
    "モデルの定義自体は基本的に前回の分類問題と同様。\n",
    "変更するのは，入出力データのサイズ。\n",
    "\n",
    "### 前回の入出力サイズ\n",
    "入力：縦横28ピクセルのモノクロ画像データのため，１ピクセル毎の輝度データが入力，つまり28×28=784個の数値を入力とした。\n",
    "\n",
    "出力：10種類の衣類の種類それぞれの確率を数字としたため，10個の数字が出力\n",
    "\n",
    "### 今回の入出力サイズ\n",
    "入力：各列のパラメータが対応するため，数値8個を入力する。X.shape[1]は行列の幅を返す。\n",
    "\n",
    "出力：回帰対象，つまり単一の数値が出力\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = X.shape[1]\n",
    "output_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 訓練に際して、可能であればGPU（cuda）を設定します。GPUが搭載されていない場合はCPUを使用します\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using {} device\".format(device))\n",
    "\n",
    "\n",
    "# modelを定義します\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(input_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, output_size),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "# モデルのインスタンスを作成\n",
    "train_error = []; test_error = []; total_epochs = 0\n",
    "parameters['model'] = NeuralNetwork(input_size=input_size, output_size=output_size).to(device)\n",
    "model = parameters.get('model')\n",
    "print(parameters['model'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 訓練条件の指定\n",
    "損失関数，最適化アルゴリズムを指定する。\n",
    "前回と違うのは，損失関数が単純に数値の誤差のため，MSELoss()を使用する点\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters['loss_fn'] = nn.MSELoss()\n",
    "parameters['learning_rate'] = 1e-2\n",
    "parameters['optimizer'] = torch.optim.Adam(model.parameters(), lr=parameters['learning_rate'])\n",
    "\n",
    "loss_fn = parameters.get('loss_fn')\n",
    "optimizer = parameters['optimizer']\n",
    "\n",
    "#学習スケジューラーを入れる場合はここに記述\n",
    "parameters['scheduler'] = torch.optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.1)\n",
    "#scheduler = parameters['scheduler']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer, scheduler = None):\n",
    "\n",
    "    model.train()\n",
    "    RMSE = 0; size = 0\n",
    "\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        size += 1\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        \n",
    "        # 損失誤差を計算\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        \n",
    "        buf_fn = nn.MSELoss(); RMSE += buf_fn(pred, y).item()        \n",
    "        \n",
    "        # バックプロパゲーション\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "    RMSE /= size\n",
    "    RMSE = RMSE**0.5\n",
    "\n",
    "    if scheduler is not None:\n",
    "        try:\n",
    "            scheduler.step(loss)\n",
    "        except:\n",
    "            scheduler.step\n",
    "\n",
    "    return RMSE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, monitor = False, range = (0,1)):\n",
    "    size = 0; RMSE = 0\n",
    "    model.eval()\n",
    "\n",
    "    if monitor:\n",
    "        predictions = []\n",
    "        truths = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            size += 1\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "\n",
    "            if monitor:\n",
    "                predictions.append(pred)\n",
    "                truths.append(y)\n",
    "\n",
    "            buf_fn = nn.MSELoss(); RMSE += buf_fn(pred, y).item()        \n",
    "            \n",
    "    RMSE /= size\n",
    "    RMSE = RMSE**0.5\n",
    "\n",
    "    if monitor:\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        predictions = torch.cat(predictions).cpu().numpy()\n",
    "        truths = torch.cat(truths).cpu().numpy()\n",
    "        print(range)\n",
    "        plt.plot([range[0],range[1]], [range[0],range[1]], color = \"black\")\n",
    "        plt.plot(truths, predictions, \"o\", color = \"black\", alpha = 0.5)\n",
    "        plt.xlabel(f\"True\")\n",
    "        plt.ylabel(f\"Predicted\")\n",
    "        #rmseをテキストとして枠内に表示\n",
    "        plt.text(0.05, 0.95, f\"RMSE: {RMSE:.2f}\", transform=plt.gca().transAxes)\n",
    "        plt.subplot(1, 2, 2)\n",
    "        ax2 = plt.gca()\n",
    "        \n",
    "        # 表示するテキストを作成\n",
    "        text = \"\"\n",
    "        for key, value in parameters.items():\n",
    "            if key == 'optimizer':\n",
    "                value =  str(value)[:5]\n",
    "            text += f\"{key}: {value}\\n\"\n",
    "\n",
    "        # Subplot内にテキストを追加\n",
    "        plt.text(0, 1, text, transform=ax2.transAxes, fontsize=10, verticalalignment='top')\n",
    "\n",
    "        # グラフのタイトルなどを必要に応じて設定\n",
    "        plt.title(\"Parameters\")\n",
    "        plt.axis('off') # 軸を非表示にする\n",
    "\n",
    "        plt.show()\n",
    "        \n",
    "    return RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters['epochs'] = 100\n",
    "total_epochs += parameters['epochs']\n",
    "\n",
    "for t in tqdm.tqdm(range(parameters['epochs'])):\n",
    "    train_error.append(train(train_loader, model, loss_fn, optimizer))\n",
    "    test_error.append(test(test_loader, model, False))\n",
    "    \n",
    "print(\"完了\")\n",
    "print(train_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習曲線の可視化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.plot(range(1,total_epochs+1), train_error, label=\"train\", color=\"black\", linestyle=\"dashed\")\n",
    "plt.plot(range(1,total_epochs+1), test_error, label=\"test\", color=\"blue\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Average RMSE\")\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 訓練データに対する予測結果を表示\n",
    "print(\"train data prediction\")\n",
    "test(train_loader, model, monitor=True, range = (df[parameters['regression_terget']].min(), df[parameters['regression_terget']].max()))\n",
    "# テストデータに対する予測結果を表示\n",
    "print(\"test data prediction\")\n",
    "test(test_loader, model, monitor=True, range = (df[parameters['regression_terget']].min(), df[parameters['regression_terget']].max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 精度向上へのヒント\n",
    "1. データの前処理は正しかったか？\n",
    "    1. 列によって値のレンジが異なるが，これを同じように入力しても大丈夫？\n",
    "    1. 合金の名前など，文字で記載された列を削除したが，文字列のデータ（カテゴリデータ）を数値に変換（リンゴ=0,バナナ=2のように置き換え）してヒントとして使えないか\n",
    "    1. 欠損データを削除したが，その他の列の数値を活用できるようにできないか（平均値で埋めてみる，0で埋めてみる。等）\n",
    "    1. そもそも，対称を予測するに適切にデータを含んでいたか？追加はできないか？[天気などは指定期間，場所でダウンロードできる](https://open-meteo.com/en/docs)\n",
    "    \n",
    "1. 学習量（epoch数）は充分か？\n",
    "    1. 学習曲線におけるテストデータの誤差が減少傾向であれば，学習不足。\n",
    "    1. テストデータよりも訓練データに対する誤差が大きい場合も，学習不足の可能性が高い。\n",
    "\n",
    "1. 学習速度係数(lr)は適切か？\n",
    "    1. 大きいと収束は早いが安定しにくい。\n",
    "    1. 小さいと収束結果はいいが学習が遅い。\n",
    "    1. 学習の途中で学習係数を調整するスケジューラーも存在する（from torch.optim import lr_scheduler）\n",
    "    \n",
    "1. batch_sizeは？\n",
    "    1. 一度に見せるデータの量，基本的には小さい方が高精度とされる\n",
    "    1. が，小さいと訓練回数が増えるので，学習に時間がかかる。\n",
    "    1. 一般論では，データサイズの1/100程度がバランスが取れているとされる。\n",
    "\n",
    "1. モデルの表現力（層の数やニューロンの数）は適切だったか？\n",
    "    1. 表現力が不足していると，データ全てを包括した特徴を充分に抽出できない。この場合，学習曲線で訓練データの誤差が一定値以上減らなくなる。\n",
    "    1. 表現力が過剰な場合，過学習（訓練データを丸暗記し，テスト結果が悪くなる）に陥りやすい。この場合，学習曲線において訓練データに対する誤差が減っているにも関わらず，テストデータに対する誤差が増えていく。\n",
    "\n",
    "1. 活性層は？\n",
    "    1. Reluは負の値に対して，傾きを持たない。パラメータを変更しても損失が変わらない状況にスタックした場合，学習が進まない。\n",
    "    1. LeakyReluは負の値にも傾きがある。\n",
    "\n",
    "1. 損失関数は適切だったか？回帰における誤差評価関数には例えば以下のような種類がある。（今回はRMSEで競うのであまり変更の意味はない）\n",
    "    1. MSE（Mean Squared Error/2乗平均誤差）：正規分布の分散に相当する。2乗を取ることで本来の値から離れるほど非線形にペナルティが大きくなる\n",
    "    1. RMSE（Root Mean Squared Error/2乗平均平方根誤差）：正規分布の標準偏差に相当する。本来の値からの距離に対して線形にペナルティを与える。\n",
    "    1. MAE（Mean Absolute Error/平均絶対値誤差）：誤差自体の平均値\n",
    "    1. LMSE(Least Mean Square Error/最小2乗平均誤差)：MSEの対数をとった関数。誤差を対数軸（比率）で評価可能になるので，何桁にも渡って分布する（対数軸で評価する）回帰対象に対して適用しやすい。\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 同条件でのスコア統計取得\n",
    "モデルの初期パラメータは，乱数で決定される。ので，同じ方法での訓練でも結果はばらつく。複数回の評価で，本質的に優れた方法かが評価できる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 30\n",
    "errors = []\n",
    "for i in range(n):\n",
    "    parameters['model'] = NeuralNetwork(input_size=input_size, output_size=output_size).to(device)\n",
    "    model = parameters.get('model')\n",
    "    parameters['optimizer'] = torch.optim.Adam(model.parameters(), lr=parameters['learning_rate'])\n",
    "    optimizer = parameters['optimizer']\n",
    "    #学習スケジューラーを入れる場合はここに記述\n",
    "\n",
    "    for t in range(parameters['epochs']):\n",
    "        train_RMSE = train(train_loader, model, loss_fn, optimizer)\n",
    "    test_RMSE = test(test_loader, model, monitor = True, range = (df[parameters['regression_terget']].min(), df[parameters['regression_terget']].max()))\n",
    "    errors.append((train_RMSE, test_RMSE))\n",
    "\n",
    "    print(f\"\\r{i+1}/{n}: train_RMSE: {train_RMSE:.2f}, test_RMSE: {test_RMSE:.2f}\", end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 繰り返した結果のバイオリンプロットを表示\n",
    "error = torch.tensor(errors)\n",
    "error = error.cpu().numpy()\n",
    "error = error \n",
    "print(error.shape)\n",
    "fig, ax = plt.subplots(figsize=(8,8))\n",
    "vio = ax.violinplot(error, showmeans=False, showextrema=False, showmedians=False)\n",
    "ax.set_xticks([1,2], labels=[\"Train\", \"Test\"])\n",
    "for body in vio['bodies']:\n",
    "    body.set_color('black')\n",
    "    body.set_alpha(0.6)\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.ylim(0, )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMSEの統計をcsvに保存\n",
    "df_error = pd.DataFrame(errors, columns=[\"Train\", \"Test\"])\n",
    "\n",
    "# 日付と時刻を取得\n",
    "from datetime import datetime\n",
    "now = datetime.now()\n",
    "timestamp = now.strftime(\"%Y%m%d_%H%M%S\")\n",
    "df_error.to_csv(f\"./data/RMSE_{timestamp}.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここまでを実行すると，繰り返し学習した結果における誤差（RMSE）の分布が\"./data/RMSE_{timestamp}.csv\"として記録される。\n",
    "\n",
    "次に，条件を変えた際の筆画を行う。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ref = pd.read_csv(\"./data/RMSE_of_Initial_Conditions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#比較用に整形\n",
    "df_ref = pd.read_csv(\"./data/RMSE_of_Initial_Conditions.csv\")\n",
    "df_ref_train = pd.DataFrame({'RMSE': df_ref['Train']})\n",
    "df_ref_train['Data Type'] = 'Train'\n",
    "df_ref_test = pd.DataFrame({'RMSE': df_ref['Test']})\n",
    "df_ref_test['Data Type'] = 'Test'\n",
    "df_ref_combined = pd.concat([df_ref_train, df_ref_test])\n",
    "df_ref_combined['Method'] = 'Initial'\n",
    "\n",
    "df_error_train = pd.DataFrame({'RMSE': df_error['Train']})\n",
    "df_error_train['Data Type'] = 'Train'\n",
    "df_error_test = pd.DataFrame({'RMSE': df_error['Test']})\n",
    "df_error_test['Data Type'] = 'Test'\n",
    "df_error_combined = pd.concat([df_error_train, df_error_test])\n",
    "df_error_combined['Method'] = 'Your Method'\n",
    "\n",
    "df_combined = pd.concat([df_error_combined, df_ref_combined])\n",
    "\n",
    "#比較\n",
    "f,ax=plt.subplots(figsize=(8,8))\n",
    "sns.violinplot(x= \"Data Type\",y=\"RMSE\",hue=\"Method\", data=df_combined, split=True, ax=ax, inner=None, palette=[\"skyblue\", \"salmon\"])\n",
    "ax.set_ylabel(\"RMSE\")\n",
    "ax.set_ylim(0,)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### プレゼンに向けて\n",
    "与えられたコードを改良し, データに対する回帰の精度を向上させ, 結果を2分のプレゼンテーションで説明せよ​。スライドには変更点と実験結果の説明を含めること​。例えば以下のような変更点が考えられる​。\n",
    "\n",
    "1. 学習条件（エポック数, 学習係数, バッチサイズ)​\n",
    "\n",
    "1. モデル条件（層の種別, 組み合わせ, モデルの構造)​\n",
    "\n",
    "1. 前処理に関する変更点（正規化, 情報の種別）​\n",
    "\n",
    "### 実験結果には以下のものを含めること​\n",
    "\n",
    "1. テストデータに対するyyplot​\n",
    "\n",
    "1. 改善後のRMSE値（可能であればバイオリンプロット）​\n",
    "\n",
    "1. 結果に対する考察​\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import SVG, display\n",
    "display(SVG(\"./figure/ルーブリック2.svg\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
